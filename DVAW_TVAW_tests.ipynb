{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNUK1vn/KQ8fIRNrCveE2u7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pydataset"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr7jjMJeGS0T",
        "outputId": "f03aba97-07e8-4ea2-b13a-8b84b37f0543"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydataset\n",
            "  Downloading pydataset-0.2.0.tar.gz (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from pydataset) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pydataset) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pydataset) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->pydataset) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->pydataset) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->pydataset) (1.17.0)\n",
            "Building wheels for collected packages: pydataset\n",
            "  Building wheel for pydataset (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pydataset: filename=pydataset-0.2.0-py3-none-any.whl size=15939415 sha256=f7ddb462de69bdef00db20a03b720ee3d555e066bbceda86178ca44ce47e098f\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/93/3f/af54c413cecaac292940342c61882d2a8848674175d0bb0889\n",
            "Successfully built pydataset\n",
            "Installing collected packages: pydataset\n",
            "Successfully installed pydataset-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TZQiMsZiF4Vt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from numpy import linalg as LA\n",
        "from matplotlib.ticker import ScalarFormatter\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, MinMaxScaler, RobustScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "import seaborn as sns\n",
        "from pydataset import data\n",
        "from sklearn.datasets import fetch_california_housing, load_diabetes, fetch_openml\n",
        "from statsmodels.datasets import get_rdataset,co2, sunspots, elnino, macrodata, nile, randhie\n",
        "from pandas_datareader import get_data_fred\n",
        "from sklearn.datasets import fetch_openml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DVAW"
      ],
      "metadata": {
        "id": "1kXQmZtmGf15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DVAW:\n",
        "    def __init__(self, n_components=50,\n",
        "                 dict_of_kernels={'Gaussian': 51, 'Laplacian': 25}):\n",
        "        # Initialize DVAW model parameters\n",
        "        self.n_components = n_components\n",
        "        self.lambda_reg = 1 # Regularization parameter\n",
        "        self.dict_of_kernels = dict_of_kernels\n",
        "        # Generate kernel parameters based on dictionary\n",
        "        self.gamma, self.kernel_list, self.num_rbf, self.num_lap, self.P = self._generate_kernel_params(dict_of_kernels)\n",
        "        self.ran_feature = None # Stores random features for kernels\n",
        "        self.experts_weights = None # Stores weights for individual kernel experts\n",
        "        self.weights_vaw2 = None # Stores weights for the second layer (VAW2 combiner)\n",
        "\n",
        "    def _generate_kernel_params(self, dict_of_kernels):\n",
        "        # Generates gamma values and kernel types for each expert\n",
        "        gamma = []\n",
        "        kernel_list = []\n",
        "        num_rbf, num_lap, = 0, 0\n",
        "        P = 0 # Total number of kernels/experts\n",
        "        for kernel, num_kernel in dict_of_kernels.items():\n",
        "            P += num_kernel # Accumulate total number of experts\n",
        "            if num_kernel > 0:\n",
        "                gamma_values = np.logspace(-2, 2, num_kernel) # Generate gamma values for each kernel instance\n",
        "                if kernel == 'Gaussian':\n",
        "                    num_rbf = num_kernel\n",
        "                    gamma.extend(gamma_values)\n",
        "                    kernel_list.extend(['Gaussian'] * num_kernel)\n",
        "                elif kernel == 'Laplacian':\n",
        "                    num_lap = num_kernel\n",
        "                    gamma.extend(gamma_values)\n",
        "                    kernel_list.extend(['Laplacian'] * num_kernel)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown kernel type: {kernel}\")\n",
        "        return np.array(gamma), kernel_list, num_rbf, num_lap, P\n",
        "\n",
        "    def generate_random_features_and_dict(self, X):\n",
        "        # Generates random Fourier features for each kernel type\n",
        "        np.random.seed(1) # For reproducibility\n",
        "        M, N = X.shape # M samples, N original features\n",
        "        self.ran_feature = np.zeros((N, self.n_components, self.P)) # Stores random projections for each kernel\n",
        "        random_features = {} # Dictionary to hold generated features for each expert\n",
        "        kernel_counters = {'Gaussian': 0, 'Laplacian': 0} # Counters for naming experts\n",
        "        for i, kernel_type in enumerate(self.kernel_list):\n",
        "            features = np.zeros((M, self.n_components * 2)) # Placeholder for current kernel's features\n",
        "            if kernel_type == 'Gaussian':\n",
        "                gamma = self.gamma[i]\n",
        "                self.ran_feature[:, :, i] = np.random.randn(N, self.n_components) * np.sqrt(1 / gamma) # Random projections for Gaussian\n",
        "                for j in range(M):\n",
        "                    X_f = X[j:j + 1, :].dot(self.ran_feature[:, :, i])\n",
        "                    features[j, :] = (1 / np.sqrt(self.n_components)) * np.concatenate((np.sin(X_f), np.cos(X_f)), axis=1) # Apply sin/cos for Fourier features\n",
        "                random_features[f\"{kernel_type}_{kernel_counters['Gaussian']}\"] = features\n",
        "                kernel_counters['Gaussian'] += 1\n",
        "            elif kernel_type == 'Laplacian':\n",
        "                gamma = self.gamma[i]\n",
        "                laplacian_index = i - self.num_rbf # Adjust index for Laplacian kernels\n",
        "                if 0 <= laplacian_index < self.num_lap:\n",
        "                    self.ran_feature[:, :, i] = np.random.standard_cauchy((N, self.n_components)) * (1 / self.gamma[i]) # Random projections for Laplacian\n",
        "                    for j in range(M):\n",
        "                        X_f = X[j:j + 1, :].dot(self.ran_feature[:, :, i])\n",
        "                        features[j, :] = (1 / np.sqrt(self.n_components)) * np.concatenate((np.sin(X_f), np.cos(X_f)), axis=1) # Apply sin/cos for Fourier features\n",
        "                    random_features[f\"{kernel_type}_{kernel_counters['Laplacian']}\"] = features\n",
        "                    kernel_counters['Laplacian'] += 1\n",
        "                else:\n",
        "                    raise IndexError(\"Laplacian index out of bounds\")\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown kernel type: {kernel_type}\")\n",
        "        return random_features\n",
        "\n",
        "    def _vaw_forecaster(self, features, target):\n",
        "        # Implements the Vovk-Azoury-Warmuth (VAW) online linear regressor\n",
        "        n_samples, n_features = features.shape\n",
        "        predictions = []\n",
        "        weights = np.zeros(n_features) # Final weights of the online algorithm\n",
        "        reg_matrix_inverse = (1 / self.lambda_reg) * np.eye(n_features) # Initialize inverse regularization matrix\n",
        "        sum_yizi = np.zeros(n_features) # Sum of target * feature vector for weight update\n",
        "        for t in range(n_samples):\n",
        "            zt = features[t] # Current feature vector\n",
        "            yt = target[t] # Current target value\n",
        "            if t == 0:\n",
        "                prediction = 0 # Initial prediction for the first sample (can be customized)\n",
        "            else:\n",
        "                prediction = np.dot(sum_yizi, reg_matrix_inverse @ zt) # Calculate prediction based on current weights\n",
        "            predictions.append(prediction)\n",
        "            sum_yizi += yt * zt # Accumulate y_t * z_t\n",
        "            zt = zt.reshape(-1, 1) # Reshape z_t for matrix operations\n",
        "            # Update inverse regularization matrix using Sherman-Morrison formula\n",
        "            numerator = reg_matrix_inverse @ zt @ zt.T @ reg_matrix_inverse\n",
        "            denominator = 1 + zt.T @ reg_matrix_inverse @ zt\n",
        "            reg_matrix_inverse = reg_matrix_inverse - (numerator / denominator[0, 0])\n",
        "        if n_samples > 0:\n",
        "            weights = reg_matrix_inverse @ sum_yizi # Compute final weights\n",
        "        return np.array(predictions), weights\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        # Fits the DVAW model by training individual experts and then a second-layer VAW combiner\n",
        "        self.fourier_features = self.generate_random_features_and_dict(X) # Generate random Fourier features for each expert\n",
        "        self.experts_weights = np.zeros((self.P, self.n_components * 2)) # Initialize weights for each expert's linear model\n",
        "        experts_predictions = pd.DataFrame() # DataFrame to store predictions from each expert\n",
        "\n",
        "        i = 0\n",
        "        for kernel_name, features in self.fourier_features.items():\n",
        "            # Train each expert (using VAW forecaster) and get its predictions and weights\n",
        "            predictions, self.experts_weights[i] = self._vaw_forecaster(features, Y)\n",
        "            experts_predictions = pd.concat([experts_predictions,\n",
        "                                             pd.DataFrame(predictions, columns=[kernel_name])], axis=1)\n",
        "            i += 1\n",
        "        # Train the second-layer VAW forecaster using expert predictions as input features\n",
        "        self.predictions_vaw2, self.weights_vaw2 = self._vaw_forecaster(np.array(experts_predictions), Y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Makes predictions on new data using the trained DVAW model\n",
        "        M, N = X.shape # M samples, N original features\n",
        "        random_features_list = [] # List to store generated random features for prediction\n",
        "        experts_predictions = np.zeros((self.P, M)) # Array to store predictions from each expert\n",
        "\n",
        "        # Generate random Fourier features for the new X data\n",
        "        for i, kernel_type in enumerate(self.kernel_list):\n",
        "            features = np.zeros((M, self.n_components * 2))\n",
        "            for j in range(M):\n",
        "                # Apply the same random projections used during training\n",
        "                X_f = X[j:j + 1, :].dot(self.ran_feature[:, :, i])\n",
        "                features[j, :] = (1 / np.sqrt(self.n_components)) * np.concatenate((np.sin(X_f), np.cos(X_f)), axis=1)\n",
        "            random_features_list.append(features)\n",
        "        random_features_array = np.array(random_features_list)\n",
        "\n",
        "        # Calculate predictions from each expert using their learned weights\n",
        "        for p in range(self.P):\n",
        "            experts_predictions[p] = np.dot(random_features_array[p], self.experts_weights[p])\n",
        "\n",
        "        # Combine expert predictions using the second-layer VAW weights\n",
        "        vaw2_predictions = experts_predictions.T @ self.weights_vaw2\n",
        "        return vaw2_predictions"
      ],
      "metadata": {
        "id": "UGmeTqLhGbP1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DVAW scaled"
      ],
      "metadata": {
        "id": "3Mw3rVSyGi9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DVAW_scaled:\n",
        "    def __init__(self, n_components=50,\n",
        "                 dict_of_kernels={'Gaussian': 51, 'Laplacian': 25},\n",
        "                 x_scaler=None):\n",
        "        \"\"\"\n",
        "        Initializes the DVAW_scaled model.\n",
        "\n",
        "        Args:\n",
        "            n_components (int): Number of random features for each kernel.\n",
        "            dict_of_kernels (dict): Dictionary specifying kernel types and their counts.\n",
        "                                    E.g., {'Gaussian': 51, 'Laplacian': 25}.\n",
        "            x_scaler (sklearn.preprocessing.Scaler, optional): Scaler for input features X.\n",
        "                                                               Defaults to StandardScaler if None.\n",
        "        \"\"\"\n",
        "        self.n_components = n_components\n",
        "        self.lambda_reg = 1 # Regularization parameter for VAW forecaster\n",
        "        self.dict_of_kernels = dict_of_kernels\n",
        "\n",
        "        # Initialize X scaler, using provided one or default StandardScaler\n",
        "        self.x_scaler = x_scaler\n",
        "\n",
        "        # Generate kernel parameters (gamma values, kernel list, counts)\n",
        "        self.gamma, self.kernel_list, self.num_rbf, self.num_lap, self.P = self._generate_kernel_params(dict_of_kernels)\n",
        "\n",
        "        self.ran_feature = None # Stores random projections for each kernel (set during fit)\n",
        "        self.experts_weights = None # Stores learned weights for individual kernel experts\n",
        "        self.weights_vaw2 = None # Stores learned weights for the second-layer VAW combiner\n",
        "\n",
        "        np.random.seed(1) # Set random seed for reproducibility\n",
        "\n",
        "    def _generate_kernel_params(self, dict_of_kernels):\n",
        "        \"\"\"\n",
        "        Generates kernel parameters (gamma values and a list of kernel types)\n",
        "        based on the provided dictionary.\n",
        "\n",
        "        Args:\n",
        "            dict_of_kernels (dict): Dictionary specifying kernel types and their counts.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (gamma_array, kernel_type_list, num_gaussian_kernels, num_laplacian_kernels, total_kernels)\n",
        "        \"\"\"\n",
        "        gamma = []\n",
        "        kernel_list = []\n",
        "        num_rbf, num_lap = 0, 0\n",
        "        P = 0 # Total number of individual kernel experts\n",
        "        for kernel, num_kernel in dict_of_kernels.items():\n",
        "            P += num_kernel\n",
        "            if num_kernel > 0:\n",
        "                # Generate gamma values logarithmically spaced\n",
        "                gamma_values = np.logspace(-2, 2, num_kernel)\n",
        "                if kernel == 'Gaussian':\n",
        "                    num_rbf = num_kernel\n",
        "                    gamma.extend(gamma_values)\n",
        "                    kernel_list.extend(['Gaussian'] * num_kernel)\n",
        "                elif kernel == 'Laplacian':\n",
        "                    num_lap = num_kernel\n",
        "                    gamma.extend(gamma_values)\n",
        "                    kernel_list.extend(['Laplacian'] * num_kernel)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown kernel type: {kernel}\")\n",
        "        return np.array(gamma), kernel_list, num_rbf, num_lap, P\n",
        "\n",
        "    def generate_random_features_and_dict(self, X):\n",
        "        \"\"\"\n",
        "        Generates random projection matrices (ran_feature_matrix) and\n",
        "        corresponding Fourier features (random_features) for the given input X.\n",
        "\n",
        "        Args:\n",
        "            X (np.array): Input features (expected to be already scaled).\n",
        "\n",
        "        Returns:\n",
        "            tuple: (random_features_dict, random_feature_matrix)\n",
        "                random_features_dict (dict): Dictionary of Fourier features for each kernel.\n",
        "                random_feature_matrix (np.array): Matrix of random projections for each kernel.\n",
        "        \"\"\"\n",
        "        M, N = X.shape # M: number of samples, N: number of original features\n",
        "\n",
        "        # ran_feature_matrix is local and will be returned and stored in self.ran_feature\n",
        "        ran_feature_matrix = np.zeros((N, self.n_components, self.P))\n",
        "        random_features = {} # Dictionary to store Fourier features for each expert\n",
        "        kernel_counters = {'Gaussian': 0, 'Laplacian': 0} # Counters for naming experts\n",
        "\n",
        "        for i, kernel_type in enumerate(self.kernel_list):\n",
        "            features = np.zeros((M, self.n_components * 2)) # Fourier features for the current kernel\n",
        "            gamma = self.gamma[i] # Gamma value for the current kernel\n",
        "\n",
        "            if kernel_type == 'Gaussian':\n",
        "                ran_feature_matrix[:, :, i] = np.random.randn(N, self.n_components) * np.sqrt(1 / gamma)\n",
        "            elif kernel_type == 'Laplacian':\n",
        "                laplacian_index = i - self.num_rbf\n",
        "                if not (0 <= laplacian_index < self.num_lap):\n",
        "                    raise IndexError(\"Laplacian index out of bounds\")\n",
        "                ran_feature_matrix[:, :, i] = np.random.standard_cauchy((N, self.n_components)) * (1 / gamma)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown kernel type: {kernel_type}\")\n",
        "\n",
        "            # Compute Fourier features for the current kernel\n",
        "            for j in range(M):\n",
        "                X_f = X[j:j + 1, :].dot(ran_feature_matrix[:, :, i])\n",
        "                features[j, :] = (1 / np.sqrt(self.n_components)) * np.concatenate((np.sin(X_f), np.cos(X_f)), axis=1)\n",
        "\n",
        "            # Add Fourier features to the dictionary\n",
        "            random_features[f\"{kernel_type}_{kernel_counters[kernel_type]}\"] = features\n",
        "            kernel_counters[kernel_type] += 1\n",
        "\n",
        "        return random_features, ran_feature_matrix\n",
        "\n",
        "    def _vaw_forecaster(self, features, target):\n",
        "        \"\"\"\n",
        "        Implements the Vovk-Azoury-Warmuth (VAW) online linear regressor.\n",
        "\n",
        "        Args:\n",
        "            features (np.array): Feature matrix (n_samples, n_features).\n",
        "            target (np.array): Target values vector (n_samples,).\n",
        "\n",
        "        Returns:\n",
        "            tuple: (predictions, weights)\n",
        "                predictions (np.array): Vector of online predictions.\n",
        "                weights (np.array): Learned weights.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = features.shape\n",
        "        predictions = []\n",
        "        weights = np.zeros(n_features) # Final weights of the online algorithm\n",
        "        reg_matrix_inverse = (1 / self.lambda_reg) * np.eye(n_features) # Initialize inverse regularization matrix\n",
        "        sum_yizi = np.zeros(n_features) # Sum of target * feature vector for weight update\n",
        "\n",
        "        for t in range(n_samples):\n",
        "            zt = features[t] # Current feature vector\n",
        "            yt = target[t] # Current target value\n",
        "\n",
        "            if t == 0:\n",
        "                prediction = 0 # Initial prediction for the first sample\n",
        "            else:\n",
        "                prediction = np.dot(sum_yizi, reg_matrix_inverse @ zt) # Calculate prediction based on current weights\n",
        "            predictions.append(prediction)\n",
        "\n",
        "            sum_yizi += yt * zt # Accumulate y_t * z_t\n",
        "            zt_reshaped = zt.reshape(-1, 1) # Reshape z_t for matrix operations\n",
        "\n",
        "            # Update inverse regularization matrix using Sherman-Morrison formula\n",
        "            numerator = reg_matrix_inverse @ zt_reshaped @ zt_reshaped.T @ reg_matrix_inverse\n",
        "            denominator = 1 + zt_reshaped.T @ reg_matrix_inverse @ zt_reshaped\n",
        "\n",
        "            # Check for division by zero, though unlikely with proper lambda_reg\n",
        "            if denominator[0, 0] == 0:\n",
        "                pass # Skipping update if denominator is zero\n",
        "            else:\n",
        "                reg_matrix_inverse = reg_matrix_inverse - (numerator / denominator[0, 0])\n",
        "\n",
        "        if n_samples > 0:\n",
        "            weights = reg_matrix_inverse @ sum_yizi # Compute final weights\n",
        "        return np.array(predictions), weights\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "        Fits the DVAW_scaled model on input data X and Y.\n",
        "\n",
        "        Args:\n",
        "            X (np.array or pd.DataFrame): Input features.\n",
        "            Y (np.array or pd.Series): Target values.\n",
        "        \"\"\"\n",
        "        # Ensure Y is a numpy array if it's a pandas Series\n",
        "        Y_np = Y.values if isinstance(Y, pd.Series) else Y\n",
        "\n",
        "        # Define training size for scaler fitting (e.g., 75% of data)\n",
        "        train_size = int(0.75 * X.shape[0])\n",
        "\n",
        "        # 1. Scale input features X\n",
        "        if self.x_scaler is not None:\n",
        "          self.x_scaler.fit(X[:train_size]) # Fit scaler only on a portion of data\n",
        "          X_scaled = self.x_scaler.transform(X) # Transform all data\n",
        "\n",
        "        else:\n",
        "          X_scaled = X\n",
        "\n",
        "        # Generate random features and Fourier features based on scaled X\n",
        "        self.fourier_features, self.ran_feature = self.generate_random_features_and_dict(X_scaled)\n",
        "\n",
        "        # Initialize weights for experts\n",
        "        self.experts_weights = np.zeros((self.P, self.n_components * 2))\n",
        "        experts_predictions = pd.DataFrame() # DataFrame to store online predictions from each expert\n",
        "\n",
        "        i = 0\n",
        "        # First layer VAW: train individual experts for each kernel\n",
        "        for kernel_name, features in self.fourier_features.items():\n",
        "            # Train experts using original Y\n",
        "            predictions, self.experts_weights[i] = self._vaw_forecaster(features, Y_np)\n",
        "            experts_predictions = pd.concat([experts_predictions,\n",
        "                                             pd.DataFrame(predictions, columns=[kernel_name])], axis=1)\n",
        "            i += 1\n",
        "\n",
        "        # Second layer VAW: combine expert predictions\n",
        "        # Train the second layer using expert online predictions and original Y\n",
        "        self.predictions_vaw2, self.weights_vaw2 = self._vaw_forecaster(np.array(experts_predictions), Y_np)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Makes predictions on new data X, including scaling of input features.\n",
        "\n",
        "        Args:\n",
        "            X (np.array or pd.DataFrame): Input features for prediction.\n",
        "\n",
        "        Returns:\n",
        "            np.array: Final model predictions, in the original Y range.\n",
        "        \"\"\"\n",
        "        # 1. Scale new input features X using the fitted x_scaler\n",
        "        if self.x_scaler is not None:\n",
        "          X_scaled = self.x_scaler.transform(X)\n",
        "        else: X_scaled = X\n",
        "\n",
        "        M, N = X_scaled.shape\n",
        "        random_features_list = [] # List to store generated random features for prediction\n",
        "\n",
        "        # Initialize experts_predictions_from_features with correct shape: (num_samples, num_experts)\n",
        "        experts_predictions_from_features = np.zeros((M, self.P))\n",
        "\n",
        "        # Generate Fourier features for each kernel based on the scaled X data\n",
        "        for i, kernel_type in enumerate(self.kernel_list):\n",
        "            features = np.zeros((M, self.n_components * 2))\n",
        "            for j in range(M):\n",
        "                # Apply the same random projections (self.ran_feature) used during training\n",
        "                X_f = X_scaled[j:j + 1, :].dot(self.ran_feature[:, :, i])\n",
        "                features[j, :] = (1 / np.sqrt(self.n_components)) * np.concatenate((np.sin(X_f), np.cos(X_f)), axis=1)\n",
        "            random_features_list.append(features)\n",
        "\n",
        "        # Calculate predictions from each expert using their learned weights\n",
        "        for p in range(self.P):\n",
        "            experts_predictions_from_features[:, p] = np.dot(random_features_list[p], self.experts_weights[p])\n",
        "\n",
        "        # Apply the second-layer VAW weights to combine expert predictions.\n",
        "        # Predictions will be in the original Y range since Y was not scaled during fit.\n",
        "        final_predictions = experts_predictions_from_features @ self.weights_vaw2\n",
        "\n",
        "        return final_predictions"
      ],
      "metadata": {
        "id": "aQgiW6KQGjIN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TVAW"
      ],
      "metadata": {
        "id": "HgRY65QDGlvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TVAW:\n",
        "    def __init__(self, n_components=50,\n",
        "                 dict_of_kernels={'Gaussian': 51, 'Laplacian': 25}):\n",
        "        self.n_components = n_components\n",
        "        self.lambda_reg = 1\n",
        "        self.dict_of_kernels = dict_of_kernels\n",
        "\n",
        "        # Initialize kernel parameters\n",
        "        self.gamma, self.kernel_list, self.num_rbf, self.num_lap, self.P = self._generate_kernel_params(dict_of_kernels)\n",
        "\n",
        "        self.final_predictions = None\n",
        "        self.final_weights = None\n",
        "\n",
        "        # List of X-scaler options, including None (no scaling)\n",
        "        self.x_scalers_options = [\n",
        "            ('StandardScaler', StandardScaler),\n",
        "            ('MinMaxScaler', MinMaxScaler),\n",
        "            ('RobustScaler', RobustScaler),\n",
        "            ('NoScaler', None)\n",
        "        ]\n",
        "        # Y-scaler class option\n",
        "        self.y_scaler_option = MinMaxScaler\n",
        "\n",
        "        # Stores trained scalers and model components for each combination\n",
        "        self.scaler_combinations = []\n",
        "\n",
        "        # Dictionary to store ALL trained scalers, random features, and weights for each combination\n",
        "        self.trained_scalers = {}\n",
        "\n",
        "        # Set random seed for reproducibility of random features\n",
        "        np.random.seed(1)\n",
        "\n",
        "        # Generate all possible X and Y scaler combinations\n",
        "        self._generate_scaler_combinations()\n",
        "\n",
        "    def _generate_kernel_params(self, dict_of_kernels):\n",
        "        \"\"\"\n",
        "        Generates kernel parameters (gamma values and kernel types).\n",
        "        \"\"\"\n",
        "        gamma = []\n",
        "        kernel_list = []\n",
        "        num_rbf, num_lap = 0, 0\n",
        "        P = 0 # Total number of kernels\n",
        "        for kernel, num_kernel in dict_of_kernels.items():\n",
        "            P += num_kernel\n",
        "            if num_kernel > 0:\n",
        "                gamma_values = np.logspace(-2, 2, num_kernel)\n",
        "                if kernel == 'Gaussian':\n",
        "                    num_rbf = num_kernel\n",
        "                    gamma.extend(gamma_values)\n",
        "                    kernel_list.extend(['Gaussian'] * num_kernel)\n",
        "                elif kernel == 'Laplacian':\n",
        "                    num_lap = num_kernel\n",
        "                    gamma.extend(gamma_values)\n",
        "                    kernel_list.extend(['Laplacian'] * num_kernel)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown kernel type: {kernel}\")\n",
        "        return np.array(gamma), kernel_list, num_rbf, num_lap, P\n",
        "\n",
        "    def _generate_scaler_combinations(self):\n",
        "        \"\"\"\n",
        "        Creates a list of all X and Y scaler combinations.\n",
        "        \"\"\"\n",
        "        y_scaler_templates = [\n",
        "            ('Yraw', None), # No scaling for Y\n",
        "            ('Yscaled', self.y_scaler_option) # MinMaxScaler for Y\n",
        "        ]\n",
        "\n",
        "        for x_name, x_scaler_class in self.x_scalers_options:\n",
        "            for y_name, y_scaler_class in y_scaler_templates:\n",
        "                # Create X-scaler instance or None\n",
        "                current_x_scaler_instance = x_scaler_class() if x_scaler_class is not None else None\n",
        "                # Create Y-scaler instance or None\n",
        "                current_y_scaler_instance = y_scaler_class() if y_scaler_class is not None else None\n",
        "\n",
        "                combo_full_name = f\"{x_name}_{y_name}\"\n",
        "                self.scaler_combinations.append((combo_full_name, current_x_scaler_instance, current_y_scaler_instance))\n",
        "\n",
        "    def generate_random_features_and_dict(self, X):\n",
        "        \"\"\"\n",
        "        Generates random projection matrices and corresponding Fourier features for X.\n",
        "        \"\"\"\n",
        "        M, N = X.shape\n",
        "        ran_feature_matrix = np.zeros((N, self.n_components, self.P))\n",
        "        random_features = {}\n",
        "        kernel_counters = {'Gaussian': 0, 'Laplacian': 0}\n",
        "\n",
        "        for i, kernel_type in enumerate(self.kernel_list):\n",
        "            features = np.zeros((M, self.n_components * 2))\n",
        "            gamma = self.gamma[i]\n",
        "\n",
        "            if kernel_type == 'Gaussian':\n",
        "                ran_feature_matrix[:, :, i] = np.random.randn(N, self.n_components) * np.sqrt(1 / gamma)\n",
        "            elif kernel_type == 'Laplacian':\n",
        "                laplacian_index = i - self.num_rbf\n",
        "                if not (0 <= laplacian_index < self.num_lap):\n",
        "                    raise IndexError(\"Laplacian index out of bounds\")\n",
        "                ran_feature_matrix[:, :, i] = np.random.standard_cauchy((N, self.n_components)) * (1 / gamma)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown kernel type: {kernel_type}\")\n",
        "\n",
        "            # Compute Fourier features for the current kernel\n",
        "            for j in range(M):\n",
        "                X_f = X[j:j + 1, :].dot(ran_feature_matrix[:, :, i])\n",
        "                features[j, :] = (1 / np.sqrt(self.n_components)) * np.concatenate((np.sin(X_f), np.cos(X_f)), axis=1)\n",
        "\n",
        "            random_features[f\"{kernel_type}_{kernel_counters[kernel_type]}\"] = features\n",
        "            kernel_counters[kernel_type] += 1\n",
        "\n",
        "        return random_features, ran_feature_matrix\n",
        "\n",
        "    def _vaw_forecaster(self, features, target):\n",
        "        \"\"\"\n",
        "        Implements the Vovk-Azoury-Warmuth (VAW) online linear regressor.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = features.shape\n",
        "        predictions = []\n",
        "\n",
        "        reg_matrix_inverse = (1 / self.lambda_reg) * np.eye(n_features)\n",
        "        sum_yizi = np.zeros(n_features)\n",
        "\n",
        "        for t in range(n_samples):\n",
        "            zt = features[t]\n",
        "            yt = target[t]\n",
        "\n",
        "            if t == 0:\n",
        "                prediction = 0\n",
        "            else:\n",
        "                prediction = np.dot(sum_yizi, reg_matrix_inverse @ zt)\n",
        "            predictions.append(prediction)\n",
        "\n",
        "            sum_yizi += yt * zt\n",
        "\n",
        "            zt_reshaped = zt.reshape(-1, 1)\n",
        "            numerator = reg_matrix_inverse @ zt_reshaped @ zt_reshaped.T @ reg_matrix_inverse\n",
        "            denominator = 1 + zt_reshaped.T @ reg_matrix_inverse @ zt_reshaped\n",
        "\n",
        "            if denominator[0, 0] == 0:\n",
        "                print(\"Warning: Denominator in VAW update is zero. Skipping update.\")\n",
        "            else:\n",
        "                reg_matrix_inverse = reg_matrix_inverse - (numerator / denominator[0, 0])\n",
        "\n",
        "        weights = np.zeros(n_features)\n",
        "        if n_samples > 0:\n",
        "            weights = reg_matrix_inverse @ sum_yizi\n",
        "\n",
        "        return np.array(predictions), weights\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "        Fits the TVAW model on input data X and Y.\n",
        "        \"\"\"\n",
        "        Y_original_np = Y.values if isinstance(Y, pd.Series) else Y\n",
        "        Y_2d_original = Y_original_np.reshape(-1, 1)\n",
        "\n",
        "        combined_predictions_for_final_layer = pd.DataFrame()\n",
        "        train_size = int(X.shape[0]*0.75)\n",
        "        # Iterate through each scaler combination (Level 2)\n",
        "        for combo_name, x_scaler_instance, y_scaler_instance in self.scaler_combinations:\n",
        "            # 1. Scale X for the current combination\n",
        "            if x_scaler_instance is not None:\n",
        "                x_scaler_instance.fit(X[:train_size])\n",
        "                X_scaled = x_scaler_instance.transform(X)\n",
        "                self.trained_scalers[f\"{combo_name}_x_scaler\"] = x_scaler_instance\n",
        "            else:\n",
        "                X_scaled = X.copy()\n",
        "                self.trained_scalers[f\"{combo_name}_x_scaler\"] = None\n",
        "\n",
        "            # 2. Scale Y for the current combination\n",
        "            if y_scaler_instance is not None:\n",
        "                y_scaler_instance.fit(Y_2d_original[:train_size])\n",
        "                Y_scaled = y_scaler_instance.transform(Y_2d_original).flatten()\n",
        "                self.trained_scalers[f\"{combo_name}_y_scaler\"] = y_scaler_instance\n",
        "            else:\n",
        "                Y_scaled = Y_original_np.copy()\n",
        "                self.trained_scalers[f\"{combo_name}_y_scaler\"] = None\n",
        "\n",
        "            # 3. Level 1 VAW: Generate random features and train experts\n",
        "            fourier_features_for_combo, ran_feature_for_combo = self.generate_random_features_and_dict(X_scaled)\n",
        "            self.trained_scalers[f\"{combo_name}_ran_feature\"] = ran_feature_for_combo\n",
        "\n",
        "            experts_weights_for_combo = np.zeros((self.P, self.n_components * 2))\n",
        "            experts_online_predictions_df = pd.DataFrame()\n",
        "\n",
        "            i = 0\n",
        "            for kernel_name, features in fourier_features_for_combo.items():\n",
        "                preds_online, experts_weights_for_combo[i] = self._vaw_forecaster(features, Y_scaled)\n",
        "                experts_online_predictions_df = pd.concat([\n",
        "                    experts_online_predictions_df,\n",
        "                    pd.DataFrame(preds_online, columns=[f\"{combo_name}_{kernel_name}\"])\n",
        "                ], axis=1)\n",
        "                i += 1\n",
        "            self.trained_scalers[f\"{combo_name}_experts_weights\"] = experts_weights_for_combo\n",
        "\n",
        "            # 4. Level 2 VAW: Combine expert predictions for the current combination\n",
        "            level2_preds_scaled, weights_vaw2_for_combo = self._vaw_forecaster(np.array(experts_online_predictions_df), Y_scaled)\n",
        "            self.trained_scalers[f\"{combo_name}_weights_vaw2\"] = weights_vaw2_for_combo\n",
        "\n",
        "            # 5. Inverse scale Level 2 predictions if Y was scaled\n",
        "            if y_scaler_instance is not None:\n",
        "                level2_preds_unscaled = y_scaler_instance.inverse_transform(level2_preds_scaled.reshape(-1, 1)).flatten()\n",
        "            else:\n",
        "                level2_preds_unscaled = level2_preds_scaled.copy()\n",
        "\n",
        "            combined_predictions_for_final_layer = pd.concat([\n",
        "                combined_predictions_for_final_layer,\n",
        "                pd.DataFrame(level2_preds_unscaled, columns=[combo_name])\n",
        "            ], axis=1)\n",
        "\n",
        "        # 6. Level 3 VAW: Final combination of all 8 predictions\n",
        "        self.final_predictions, self.final_weights = self._vaw_forecaster(\n",
        "            np.array(combined_predictions_for_final_layer), Y_original_np\n",
        "        )\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Makes predictions on new data X.\n",
        "        \"\"\"\n",
        "        combined_predictions_for_final_layer_predict = pd.DataFrame()\n",
        "\n",
        "        # Iterate through each scaler combination, using stored trained objects\n",
        "        for combo_name, _, _ in self.scaler_combinations:\n",
        "            # Retrieve trained scalers and components for the current combination\n",
        "            x_scaler_trained = self.trained_scalers.get(f\"{combo_name}_x_scaler\")\n",
        "            y_scaler_trained = self.trained_scalers.get(f\"{combo_name}_y_scaler\")\n",
        "            ran_feature_for_combo = self.trained_scalers.get(f\"{combo_name}_ran_feature\")\n",
        "            experts_weights_for_combo = self.trained_scalers.get(f\"{combo_name}_experts_weights\")\n",
        "            weights_vaw2_for_combo = self.trained_scalers.get(f\"{combo_name}_weights_vaw2\")\n",
        "\n",
        "            if any(item is None for item in [ran_feature_for_combo, experts_weights_for_combo, weights_vaw2_for_combo]):\n",
        "                # If basic components are missing, this combination cannot predict correctly.\n",
        "                combined_predictions_for_final_layer_predict = pd.concat([\n",
        "                    combined_predictions_for_final_layer_predict,\n",
        "                    pd.DataFrame(np.zeros(len(X)), columns=[combo_name])\n",
        "                ], axis=1)\n",
        "                continue\n",
        "\n",
        "            # 1. Scale X for prediction\n",
        "            if x_scaler_trained is not None:\n",
        "                X_scaled = x_scaler_trained.transform(X)\n",
        "            else:\n",
        "                X_scaled = X.copy()\n",
        "\n",
        "            M, N = X_scaled.shape\n",
        "            experts_predictions_from_features = np.zeros((M, self.P))\n",
        "            random_features_list = []\n",
        "\n",
        "            # Generate Fourier features for each kernel\n",
        "            for i, kernel_type in enumerate(self.kernel_list):\n",
        "                features = np.zeros((M, self.n_components * 2))\n",
        "                for j in range(M):\n",
        "                    X_f = X_scaled[j:j + 1, :].dot(ran_feature_for_combo[:, :, i])\n",
        "                    features[j, :] = (1 / np.sqrt(self.n_components)) * np.concatenate(\n",
        "                        (np.sin(X_f), np.cos(X_f)), axis=1\n",
        "                    )\n",
        "                random_features_list.append(features)\n",
        "\n",
        "            # Calculate predictions from each expert\n",
        "            for p in range(self.P):\n",
        "                experts_predictions_from_features[:, p] = np.dot(random_features_list[p], experts_weights_for_combo[p])\n",
        "\n",
        "            # 2. Apply Level 2 VAW weights. Predictions will be in the Y_scaled scale (if Y was scaled)\n",
        "            level2_preds_scaled = experts_predictions_from_features @ weights_vaw2_for_combo\n",
        "\n",
        "            # 3. Inverse scale Level 2 predictions if Y was scaled\n",
        "            if y_scaler_trained is not None:\n",
        "                level2_preds_unscaled = y_scaler_trained.inverse_transform(level2_preds_scaled.reshape(-1, 1)).flatten()\n",
        "            else:\n",
        "                level2_preds_unscaled = level2_preds_scaled.copy()\n",
        "\n",
        "            combined_predictions_for_final_layer_predict = pd.concat([\n",
        "                combined_predictions_for_final_layer_predict,\n",
        "                pd.DataFrame(np.zeros(len(X)) if level2_preds_unscaled is None else level2_preds_unscaled, columns=[combo_name])\n",
        "            ], axis=1)\n",
        "\n",
        "\n",
        "        # 4. Final prediction - combine all 8 predictions using final_weights\n",
        "        final_pred = combined_predictions_for_final_layer_predict.values @ self.final_weights\n",
        "\n",
        "        return final_pred"
      ],
      "metadata": {
        "id": "YhF6SEFSGl45"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing of all used datasets in the tests (uncomment the nessesary)"
      ],
      "metadata": {
        "id": "SBWH0LS9Jl-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------Airfoil--------\n",
        "'''\n",
        "df = pd.DataFrame(np.genfromtxt('airfoil_self_noise.dat'))\n",
        "target = 5\n",
        "'''\n",
        "\n",
        "# -------Concrete--------\n",
        "'''\n",
        "df = pd.read_excel('Concrete_Data.xls')\n",
        "target = 'Concrete compressive strength(MPa, megapascals) '\n",
        "'''\n",
        "\n",
        "#--------Bias-----------\n",
        "'''\n",
        "data = np.genfromtxt('Bias_correction_ucl.csv', skip_header=1, delimiter=',')\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "imputer = imputer.fit(data)\n",
        "df = pd.DataFrame(imputer.transform(data))\n",
        "df.drop(22, axis = 1, inplace=True)\n",
        "target = 23\n",
        "'''\n",
        "\n",
        "#----------Naval------------\n",
        "'''\n",
        "data = np.genfromtxt('NavalData.txt')\n",
        "df = pd.DataFrame(data)\n",
        "target = 0\n",
        "'''\n",
        "\n",
        "# ----------Mercedes-Benz Greener Manufacturing--------------\n",
        "'''\n",
        "df = pd.read_csv('train.csv')\n",
        "df = df.drop('ID', axis = 1)\n",
        "target = 'y'\n",
        "\n",
        "categorical_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
        "df = pd.get_dummies(df, columns=categorical_cols, prefix=categorical_cols, dtype = int)\n",
        "'''\n",
        "\n",
        "#-----------house-prices-advanced-regression-techniques-------------------\n",
        "'''\n",
        "df = pd.read_csv('train.csv')\n",
        "df.drop('Id', axis = 1, inplace = True)\n",
        "target = 'SalePrice'\n",
        "float_cols = [col for col in df.columns if df[col].dtype == float]\n",
        "\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "df[float_cols] = imputer.fit_transform(df[float_cols])\n",
        "\n",
        "categorical_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
        "for col in categorical_cols:\n",
        "  df[col] = df[col].fillna('NaN')\n",
        "df = pd.get_dummies(df, columns=categorical_cols, prefix=categorical_cols, dtype = int)\n",
        "'''\n",
        "\n",
        "\n",
        "#-----------------Behavior of the urban traffic of the city of Sao Paulo in Brazil------------------------\n",
        "'''\n",
        "df = pd.read_csv('Behavior of the urban traffic of the city of Sao Paulo in Brazil.csv', sep =';')\n",
        "target = df.columns[-1]\n",
        "df[target] = df[target].astype('string').apply(lambda x: float(x.replace(',', '.')))\n",
        "'''\n",
        "\n",
        "#---------Diamonds-----------\n",
        "'''\n",
        "df = sns.load_dataset('diamonds').sample(500, random_state=42)\n",
        "target = 'price'\n",
        "cat_cols = ['cut', 'color', 'clarity']\n",
        "df = pd.get_dummies(df, columns=cat_cols, prefix=cat_cols, dtype = int)\n",
        "'''\n",
        "\n",
        "#---------Tips-----------\n",
        "\n",
        "df = sns.load_dataset('tips')\n",
        "target = 'tip'\n",
        "\n",
        "categorical_features = ['sex', 'smoker', 'day', 'time']\n",
        "\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
        "X_cat_encoded = encoder.fit_transform(df[categorical_features])\n",
        "\n",
        "encoded_cat_columns = encoder.get_feature_names_out(categorical_features)\n",
        "X_cat_encoded_df = pd.DataFrame(X_cat_encoded, columns=encoded_cat_columns)\n",
        "\n",
        "numeric_features = ['total_bill', 'size', 'tip']\n",
        "df = pd.concat([df[numeric_features], X_cat_encoded_df], axis=1)\n",
        "\n",
        "#---------Boston-----------\n",
        "'''\n",
        "target = 'medv'\n",
        "df = data('Boston')\n",
        "'''\n",
        "#---------California-----------\n",
        "'''\n",
        "target = 'MedHouseVal'\n",
        "df = fetch_california_housing(as_frame=True).frame.iloc[:10000]\n",
        "'''\n",
        "\n",
        "#---------Mtcars-----------\n",
        "'''\n",
        "target = 'mpg'\n",
        "df = get_rdataset('mtcars').data\n",
        "'''\n",
        "\n",
        "#---------Energy Efficiency-----------\n",
        "'''\n",
        "data = fetch_openml(name='energy_efficiency', as_frame=True, parser='auto')\n",
        "target = 'Y1'\n",
        "df = data.frame\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lY3gaAYFJmKq",
        "outputId": "18b1178f-6372-4a5c-befd-cb9472c8527e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndata = fetch_openml(name='energy_efficiency', as_frame=True, parser='auto')\\ntarget = 'Y1'\\ndf = data.frame\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the DVAW model (usage is similar for DVAW_scaled and TVAW)"
      ],
      "metadata": {
        "id": "GsFyBrXrMX7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializating data\n",
        "X = df.drop(target, axis = 1).values\n",
        "Y = df[target].values\n",
        "\n",
        "train_size = int(df.shape[0]*0.75)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_test = X[train_size:]\n",
        "Y_test = Y[train_size:]\n",
        "\n",
        "# Train the model\n",
        "predictor = DVAW()\n",
        "predictor.fit(X, Y)\n",
        "\n",
        "#Make predictions on test\n",
        "preds = predictor.predict(X_test)\n",
        "\n",
        "# Print MSE\n",
        "print('MSE is ', MSE(preds, Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmKBtVeSMXGY",
        "outputId": "a70b4a28-2c7c-47b9-88da-2b6bef6d5d94"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE is  1.1927428954550534\n"
          ]
        }
      ]
    }
  ]
}